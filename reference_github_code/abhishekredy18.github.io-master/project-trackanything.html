<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TrackAnything Project - Abhishek Reddy Malreddy</title>
    <link rel="stylesheet" href="css/style.css">
</head>

<body>
    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle dark mode">üåô</button>
    <a href="projects.html" class="back-link">‚Üê Back to Projects</a>

    <h1>TrackAnything: Multi-modal Promptable Tracking in 3D</h1>

    <div class="content-section">
        <span class="date">Jan 2025 ‚Äì Apr 2025 | Carnegie Mellon University</span>
        <span class="project-tech">Computer Vision ‚Ä¢ Transformers ‚Ä¢ LiDAR ‚Ä¢ Autonomous Driving</span>
    </div>

    <div class="project-image-container">
        <img src="images/projects/trackanything.png" alt="TrackAnything 3D Tracking Visualization"
            class="project-image">
    </div>

    <div class="content-section">
        <h2>Project Overview</h2>
        <p>
            Developed an advanced ML model to generate 3D bounding boxes from text prompts and video for autonomous
            driving applications. The system enables scenario mining by allowing natural language queries like
            "walking between parked vehicles" to identify and track relevant objects in driving scenes.
        </p>
    </div>

    <div class="content-section">
        <h2>Technical Approach</h2>
        <p>
            The project integrates multiple data modalities to achieve robust 3D object tracking:
        </p>
        <ul>
            <li><strong>Multi-modal Fusion:</strong> Integrated LiDAR point clouds, HD map data, and vehicle pose
                information for comprehensive scene understanding</li>
            <li><strong>Transformer Architecture:</strong> Implemented a transformer-based architecture inspired by
                PromptTrack for text-to-3D-bbox generation</li>
            <li><strong>Dataset:</strong> Utilized the Argoverse 2 dataset, which provides rich sensor data from
                real-world urban driving scenarios</li>
            <li><strong>Natural Language Processing:</strong> Enabled text-based prompting for intuitive scenario
                specification and object querying</li>
        </ul>
    </div>

    <div class="content-section">
        <h2>Key Achievements</h2>
        <ul>
            <li>Enhanced scenario mining capabilities for self-driving applications by improving object tracking
                accuracy in complex urban environments</li>
            <li>Successfully demonstrated text-prompt driven 3D object detection and tracking across video sequences
            </li>
            <li>Integrated multiple sensor modalities (camera, LiDAR, maps) for robust performance in diverse
                conditions</li>
            <li>Enabled automation of safety-critical scenario identification for autonomous vehicle testing and
                validation</li>
        </ul>
    </div>

    <div class="content-section">
        <h2>Technologies Used</h2>
        <div class="skills-grid">
            <div class="skill-item">Python</div>
            <div class="skill-item">PyTorch</div>
            <div class="skill-item">Transformers</div>
            <div class="skill-item">LiDAR Processing</div>
            <div class="skill-item">Computer Vision</div>
            <div class="skill-item">3D Object Detection</div>
            <div class="skill-item">NLP</div>
            <div class="skill-item">Argoverse 2</div>
        </div>
    </div>

    <script src="js/script.js"></script>
</body>

</html>